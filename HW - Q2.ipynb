{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d209f4",
   "metadata": {},
   "source": [
    "# 2 The gambler problem (3 pts.)\n",
    "\n",
    "Consider the following problem, introduced in Chapter 4 of the Sutton and Barto book.  \n",
    "\n",
    "A gambler is engaged in a betting game, where he must place bets on the outcomes of a sequence of coin\n",
    "flips. Before each flip, the gambler decides how much to bet on the outcome that the coin will come up\n",
    "heads—note that he can only decide how much to bet, not in which outcome to bet.  \n",
    "\n",
    "If the coin does come up heads, the gambler doubles the money bet on that coin flip—in other words,\n",
    "if the gambler bets $5 dollars, he will get his 5 dollars back plus another 5 dollars. If the coin comes up\n",
    "tails, the gambler loses the money he bet. The game goes on until either the gambler reaches his goal of\n",
    "100 dollars, or loses by running out of money. On each flip, the gambler must decide what portion of his\n",
    "capital to stake, in integer numbers of dollars. Suppose that the probability of the coin coming up heads is\n",
    "pH = 0.4.  \n",
    "\n",
    "You will analyze the optimal betting policy for the gambler. To do so, model the gambler’s decision\n",
    "problem as an MDP, specifying the state and action spaces, the transition probabilities and the reward\n",
    "function. Make sure to use a reward function that only rewards the gambler for reaching his goal. Use a\n",
    "discount γ = 1.  \n",
    "\n",
    "**Question 2. Using the MDP model for the gambler problem, run value iteration. Plot, in the\n",
    "same plot, the computed estimate for the optimal value function at iterations 1, 2, 3 and final (stop your\n",
    "algorithm when the overall error is smaller than 10−8). Can you provide an interpretation for the values\n",
    "obtained? Plot also the optimal policy computed.**  \n",
    "\n",
    "Note: The gambler problem is somewhat numerically unstable, so to compute the optimal policy make\n",
    "sure to rounded up all values to 4 decimal places.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e164ce2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T14:48:40.396852Z",
     "start_time": "2021-10-29T14:48:40.384133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful links:\n",
    "\n",
    "# http://www.incompleteideas.net/book/first/ebook/node44.html\n",
    "# We can initialize the problem as undiscounted and having only the goal state with reward 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00af484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T14:23:50.840727Z",
     "start_time": "2021-10-29T14:23:50.074674Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4e24f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:54:02.794301Z",
     "start_time": "2021-10-29T16:54:02.746918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "[0.00000000e+00 5.07227048e-08 0.00000000e+00 5.07227048e-08\n",
      " 0.00000000e+00 5.07227048e-08 0.00000000e+00 4.86937966e-08\n",
      " 5.51863028e-09 5.47805212e-08 1.86659554e-08 7.81535435e-08\n",
      " 3.26897688e-08 3.04336229e-08 3.04336229e-08 3.04336229e-08\n",
      " 2.92162779e-08 3.28683127e-08 4.68921261e-08 1.82601737e-08\n",
      " 1.82601737e-08 1.97209876e-08 1.09561042e-08 1.18325926e-08\n",
      " 7.09955553e-09 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.31117814e-09 1.11995732e-08 1.96138613e-08\n",
      " 1.82601737e-08 1.75297667e-08 2.81352757e-08 1.09561042e-08\n",
      " 6.57366253e-09 4.25973334e-09 0.00000000e+00 0.00000000e+00\n",
      " 6.71974393e-09 1.09561042e-08 1.68811655e-08 3.94419752e-09\n",
      " 0.00000000e+00 4.03184641e-09 1.01286993e-08 0.00000000e+00\n",
      " 6.07721962e-09 3.64633174e-09 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.31117816e-09 1.11995732e-08\n",
      " 1.96138613e-08 1.82601738e-08 1.75297667e-08 2.81352757e-08\n",
      " 1.09561042e-08 6.57366256e-09 4.25973329e-09 0.00000000e+00\n",
      " 0.00000000e+00 6.71974398e-09 1.09561042e-08 1.68811655e-08\n",
      " 3.94419752e-09 0.00000000e+00 4.03184641e-09 1.01286993e-08\n",
      " 0.00000000e+00 6.07721962e-09 3.64633168e-09 0.00000000e+00\n",
      " 0.00000000e+00 1.98670680e-09 1.17683168e-08 1.05178600e-08\n",
      " 6.57366250e-09 2.55583998e-09 0.00000000e+00 6.57366261e-09\n",
      " 2.36651843e-09 2.41910780e-09 0.00000000e+00 2.18779905e-09\n",
      " 0.00000000e+00 7.06099001e-09 3.94419741e-09 0.00000000e+00\n",
      " 1.41991097e-09 0.00000000e+00 0.00000000e+00 2.36651843e-09\n",
      " 8.51946624e-10 0.00000000e+00 5.11167997e-10 3.06700776e-10\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Define a gambler class\n",
    "\n",
    "class Gambler(object):\n",
    "    \n",
    "    def __init__(self, budget_goal=100, prob_heads=0.4, discount=1.0, error=10e-8):\n",
    "        \n",
    "        # Initialize the target budget\n",
    "        self.goal = int(budget_goal)\n",
    "        \n",
    "        # Initialize the probability of winning the coin flip\n",
    "        self.prob_win = float(prob_heads)\n",
    "        \n",
    "        # Possible states: every value between 0 and 100, including\n",
    "        self.states = np.arange(budget_goal + 1)\n",
    "        \n",
    "        # State values - Budget goal has maximum value\n",
    "        self.state_values = np.zeros(budget_goal + 1)\n",
    "        self.state_values[budget_goal] = 1\n",
    "        \n",
    "        # Discount factor. 1 is undiscounted\n",
    "        self.discount = float(discount)\n",
    "        \n",
    "        # Error initialization\n",
    "        self.error = float(error)\n",
    "        \n",
    "        \n",
    "    def value_iteration(self):\n",
    "        \n",
    "        # Value iteration\n",
    "        while True:\n",
    "            delta = np.zeros(len(self.states))\n",
    "\n",
    "            # We don't want to bet 0\n",
    "            for state in self.states[1:]:\n",
    "                current_value = self.state_values[state]\n",
    "                #print(state)\n",
    "\n",
    "                # Possible actions: betting between 0 and current budget (state)\n",
    "                # If we have 95, we want to bet only 5 at max (goal - current state)\n",
    "                current_actions = np.arange(min(state, self.goal - state) + 1)\n",
    "                # print('State: {} \\n Possible bets: {}'.format(state, current_actions))\n",
    "\n",
    "                # What are the states we can end up in?\n",
    "                # I.e. if we have 10 dollars, we can end up from any value from 0 to 10 + bet_amount\n",
    "                # Calculated by following the transitions to the immediate next states.\n",
    "                # I.e. sum of current action * the probability of following it and alternative action + probability of follow\n",
    "\n",
    "                # We get an array of values for each next possible state\n",
    "                state_prime_values = []\n",
    "                for action in current_actions:\n",
    "                    # the state we end up in is either budget + bet or budget - bet\n",
    "                    # in this case is 0.4 * (budget + bet) or 0.6 * (budget - bet)\n",
    "                    state_prime_win = self.prob_win * self.state_values[state + action]\n",
    "                    state_prime_lose = (1 - self.prob_win) * self.state_values[state - action]\n",
    "                    state_prime = state_prime_win + state_prime_lose\n",
    "\n",
    "                    #print('State prime: {}'.format(state_prime))\n",
    "\n",
    "                    # Add to possible state values\n",
    "                    state_prime_values.append(state_prime)\n",
    "\n",
    "                best_value = np.max(state_prime_values)\n",
    "                #print(best_value)\n",
    "\n",
    "                # Calculate the error for finishing the iteration\n",
    "                current_delta = np.abs(current_value - best_value)\n",
    "                delta[state] = max(delta[state], current_delta)\n",
    "                #print(delta)\n",
    "                \n",
    "                # Update the value of the current state\n",
    "                self.state_values[state] = best_value\n",
    "                \n",
    "                history = delta\n",
    "\n",
    "            if max(delta) < self.error:\n",
    "                break\n",
    "                \n",
    "        print(history)\n",
    "                \n",
    "            \n",
    "    def optimal_policy(self):\n",
    "        # Get the policy\n",
    "        # Optimal policy can be found by maximizing over q*(s, a)\n",
    "        # We already have values of states, found by value iteration\n",
    "        # v*(s) = max q*(s, a) over actions\n",
    "        # Just need to pick the best value at each step\n",
    "        \n",
    "        policy_state = []\n",
    "        for state in self.states:\n",
    "            # Get the action space for each state\n",
    "            current_actions = np.arange(min(state, self.goal - state) + 1)\n",
    "            \n",
    "            # Get the updated value for each action - calculated from value iteration\n",
    "            for action in current_actions:\n",
    "                \n",
    "            \n",
    "\n",
    "gambler = Gambler()\n",
    "gambler.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef07e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
